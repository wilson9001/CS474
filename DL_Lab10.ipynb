{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Lab10",
      "provenance": [],
      "collapsed_sections": [
        "sBOvJdJfkXIL",
        "kFoEeTYHDq2s",
        "7z6g7a_Y84n0",
        "TBigIUFTukeJ",
        "zcz0JGXjxFGe",
        "2vJVbYcAJAf2",
        "gMOzGDND9FD1",
        "eOtl8z8G9wbr",
        "2Krh0eYy18R9",
        "aEDv_-H7BvM0",
        "YH5mQBaa-_0b",
        "XMKRI77_-8nc",
        "qBT5jgifC7Im",
        "_K7F19SPQo6U",
        "9YLXvK51RnuL",
        "AHmjSVf_FNHv",
        "gPXJkNubFyY6",
        "iD45m3IwF9hh",
        "usQE-rSPZq_X",
        "IoA0tZZCa_1k",
        "8pa5vFJ5EUjv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT",
        "colab_type": "text"
      },
      "source": [
        "<a \n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj",
        "colab_type": "text"
      },
      "source": [
        "#Lab 10: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL",
        "colab_type": "text"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC",
        "colab_type": "text"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzRORuLBNLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54",
        "colab_type": "text"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm",
        "colab_type": "text"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/akash2907/bird-species-classification\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ",
        "colab_type": "text"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ",
        "colab_type": "text"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhjc0pM7jOoZ",
        "colab_type": "code",
        "outputId": "b420614a-88c4-4ec6-ee3b-db40d64d64f2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-983115ed-36ed-4df5-b3c5-89b44e78ef38\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-983115ed-36ed-4df5-b3c5-89b44e78ef38\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 65 Nov 14 21:58 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB",
        "colab_type": "text"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HtB-XdIr1EE",
        "colab_type": "code",
        "outputId": "41cdf435-e6d2-4d3c-aade-5bd8c114c0a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "!kaggle datasets download -d akash2907/bird-species-classification"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading bird-species-classification.zip to /content\n",
            " 99% 1.37G/1.37G [00:32<00:00, 39.7MB/s]\n",
            "100% 1.37G/1.37G [00:32<00:00, 45.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe",
        "colab_type": "text"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and make your own."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lthPlsGeK4CX",
        "colab_type": "code",
        "outputId": "895295d6-dbfe-425f-bd75-f067daeac27b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, zip_file='bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "        super(BirdDataset, self).__init__()\n",
        "        \n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "            \n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_folder)\n",
        "\n",
        "bird_data = BirdDataset()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting bird-species-classification.zip\n",
            "Resizing images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jHFdToeDtIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########################\n",
        "# Implement your own Dataset\n",
        "#########################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG",
        "colab_type": "text"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr",
        "colab_type": "text"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY-XU4Mwas0j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNetBirds(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(ResNetBirds, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            # Turn off all gradients of the resnet\n",
        "        \n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        pass\n",
        "        \n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yOGrrw2gbIPf",
        "colab": {}
      },
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "    \n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch \n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.mean(torch.Tensor(val_losses)), torch.mean(torch.Tensor(val_accs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKESMcKi2E_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 32\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    train_dataset = BirdDataset(upload=True, train=True)\n",
        "    val_dataset = BirdDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    \n",
        "    # Model\n",
        "    model = ResNetBirds(num_classes, start_frozen=start_frozen).to(device)\n",
        "    \n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    \n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            pass\n",
        "        \n",
        "        for x, y_truth in train_loader:\n",
        "        \n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc)\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnxeLotchiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(start_frozen=False, model_unfreeze=0)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b",
        "colab_type": "text"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF",
        "colab_type": "text"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz",
        "colab_type": "text"
      },
      "source": [
        "- Modify your model class by setting the `requires_grad` attribute of the ResNet to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class (unfreeze one layer every epoch).\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im",
        "colab_type": "text"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "############################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt",
        "colab_type": "text"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE",
        "colab_type": "text"
      },
      "source": [
        "##2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL",
        "colab_type": "text"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Generate text from an the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL",
        "colab_type": "text"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDNOb_H5IRvH",
        "colab_type": "code",
        "outputId": "ee556e47-1467-42f2-f485-c4218c9e55be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version \n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "%tensorflow_version 1.x \n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.11.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.9.11)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aRJ-c9uRMOa",
        "colab_type": "code",
        "outputId": "488e8ee2-a7a4-4ec5-ac69-9db23acbb0c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 329Mit/s]                                                      "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 124M model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fetching encoder.json: 1.05Mit [00:00, 107Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 921Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:05, 87.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 278Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 154Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 205Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n",
            "Posted on November 24, 2012 | By Nick Huebner\n",
            "\n",
            "I was in a long line of supporters. I thought it was a good thing. When my three sons were born, I thought it was going to be incredible. I was just happy to have them at home with their moms. We've spoken about our anxieties and worries. The more I thought about it, the more I felt I blew it.\n",
            "\n",
            "It's been a while since I've been back to my new home a home from the top down. It's been a long time since I've been here. I've been living with my brother and dad. I've been reading a lot of books. I was just trying to get started, and really wanted to make an impact on the community.\n",
            "\n",
            "But just like we do, we can't keep going forever. We've been doing this since day one. These things are crucial. These things will never happen without them. I know we've got to feel good about ourselves and our family.\n",
            "\n",
            "I'm doing my best to make sure we stay true to our ideals.\n",
            "\n",
            "I'm not going to change. I'm not going to let any political or religious leaders take me out of the picture. We just need to go back to our roots. That's all I can think about right now.\n",
            "\n",
            "I'm proud of you. I'm proud of the people who have made us who we are. I really appreciate you.\n",
            "\n",
            "I'll see you soon.\n",
            "\n",
            "Doug\n",
            "\n",
            "Doug Huebner is Senior Staff Writer for the Illinois State Journal.\n",
            "\n",
            "(Photo: Courtesy of the Illinois State Journal)\n",
            "\n",
            "MORE FROM THE STATE JOURNAL:\n",
            "\n",
            "State Journal: Fights until 6:30 a.m. at venue\n",
            "\n",
            "Indiana State Journal: How to Make Your Own Beer\n",
            "\n",
            "Buy Photo Indiana State Journal's Bob Stokes and Jim Liu live at the Residence Inn in Jenworth on Saturday, Nov. 24, 2012. (Photo: Samuel Atman/The Register)\n",
            "\n",
            "Read or Share this story: http://indy.st/2bVjBxH<|endoftext|>The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store. The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "Properties available on the Amazon.com Store\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.com store.\n",
            "\n",
            "The aria of the two-headed lion is currently available for purchase from the Amazon.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb",
        "colab_type": "text"
      },
      "source": [
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh",
        "colab_type": "text"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESltl2QM5nxw",
        "colab_type": "code",
        "outputId": "d79b5612-2efb-4ab1-c156-0e7807f27140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "url = 'https://www.gutenberg.org/files/30/30.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/30.txt /data/text/bible.txt\n",
        "!ls ../data/text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat '/data/text/30.txt': No such file or directory\n",
            "bible.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k",
        "colab_type": "text"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP",
        "colab_type": "text"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv",
        "colab_type": "text"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuQ5snl4LuS0",
        "colab_type": "code",
        "outputId": "5b5d7e2f-4dae-4313-a54b-1254b70d6ebe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='../data/text/bible.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:06<00:00,  6.59s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 1788810 tokens\n",
            "Training...\n",
            "[1 | 7.07] loss=2.26 avg=2.26\n",
            "[2 | 8.31] loss=2.31 avg=2.29\n",
            "[3 | 9.55] loss=2.56 avg=2.38\n",
            "[4 | 10.78] loss=2.23 avg=2.34\n",
            "[5 | 12.02] loss=1.98 avg=2.27\n",
            "[6 | 13.25] loss=2.03 avg=2.22\n",
            "[7 | 14.49] loss=2.04 avg=2.20\n",
            "[8 | 15.72] loss=1.93 avg=2.16\n",
            "[9 | 16.95] loss=1.73 avg=2.11\n",
            "[10 | 18.18] loss=2.00 avg=2.10\n",
            "[11 | 19.41] loss=1.93 avg=2.09\n",
            "[12 | 20.64] loss=2.05 avg=2.08\n",
            "[13 | 21.87] loss=1.98 avg=2.07\n",
            "[14 | 23.11] loss=1.90 avg=2.06\n",
            "[15 | 24.34] loss=1.74 avg=2.04\n",
            "[16 | 25.57] loss=1.78 avg=2.02\n",
            "[17 | 26.81] loss=1.90 avg=2.01\n",
            "[18 | 28.04] loss=1.86 avg=2.00\n",
            "[19 | 29.27] loss=1.70 avg=1.99\n",
            "[20 | 30.51] loss=1.96 avg=1.98\n",
            "[21 | 31.74] loss=1.82 avg=1.98\n",
            "[22 | 32.98] loss=1.73 avg=1.96\n",
            "[23 | 34.21] loss=1.88 avg=1.96\n",
            "[24 | 35.45] loss=1.76 avg=1.95\n",
            "[25 | 36.68] loss=1.77 avg=1.94\n",
            "[26 | 37.91] loss=1.90 avg=1.94\n",
            "[27 | 39.14] loss=1.77 avg=1.93\n",
            "[28 | 40.37] loss=1.87 avg=1.93\n",
            "[29 | 41.61] loss=1.77 avg=1.92\n",
            "[30 | 42.84] loss=1.71 avg=1.92\n",
            "[31 | 44.08] loss=1.78 avg=1.91\n",
            "[32 | 45.31] loss=1.85 avg=1.91\n",
            "[33 | 46.54] loss=1.71 avg=1.90\n",
            "[34 | 47.76] loss=1.76 avg=1.90\n",
            "[35 | 49.00] loss=1.81 avg=1.89\n",
            "[36 | 50.22] loss=1.78 avg=1.89\n",
            "[37 | 51.46] loss=1.70 avg=1.88\n",
            "[38 | 52.71] loss=1.82 avg=1.88\n",
            "[39 | 53.96] loss=1.81 avg=1.88\n",
            "[40 | 55.19] loss=1.84 avg=1.88\n",
            "[41 | 56.42] loss=1.65 avg=1.87\n",
            "[42 | 57.66] loss=1.78 avg=1.87\n",
            "[43 | 58.89] loss=1.66 avg=1.86\n",
            "[44 | 60.12] loss=1.74 avg=1.86\n",
            "[45 | 61.36] loss=1.79 avg=1.86\n",
            "[46 | 62.59] loss=1.70 avg=1.85\n",
            "[47 | 63.82] loss=1.71 avg=1.85\n",
            "[48 | 65.06] loss=1.69 avg=1.85\n",
            "[49 | 66.30] loss=1.69 avg=1.84\n",
            "[50 | 67.54] loss=1.72 avg=1.84\n",
            "[51 | 68.77] loss=1.74 avg=1.84\n",
            "[52 | 70.00] loss=1.73 avg=1.83\n",
            "[53 | 71.23] loss=1.92 avg=1.84\n",
            "[54 | 72.47] loss=1.74 avg=1.83\n",
            "[55 | 73.70] loss=1.62 avg=1.83\n",
            "[56 | 74.93] loss=1.82 avg=1.83\n",
            "[57 | 76.16] loss=1.68 avg=1.82\n",
            "[58 | 77.41] loss=1.78 avg=1.82\n",
            "[59 | 78.64] loss=1.73 avg=1.82\n",
            "[60 | 79.87] loss=1.59 avg=1.82\n",
            "[61 | 81.10] loss=1.77 avg=1.82\n",
            "[62 | 82.33] loss=1.70 avg=1.81\n",
            "[63 | 83.56] loss=1.68 avg=1.81\n",
            "[64 | 84.79] loss=1.75 avg=1.81\n",
            "[65 | 86.03] loss=1.49 avg=1.80\n",
            "[66 | 87.27] loss=1.74 avg=1.80\n",
            "[67 | 88.51] loss=1.77 avg=1.80\n",
            "[68 | 89.74] loss=1.77 avg=1.80\n",
            "[69 | 90.97] loss=1.67 avg=1.80\n",
            "[70 | 92.20] loss=1.60 avg=1.79\n",
            "[71 | 93.44] loss=1.70 avg=1.79\n",
            "[72 | 94.69] loss=1.68 avg=1.79\n",
            "[73 | 95.92] loss=1.76 avg=1.79\n",
            "[74 | 97.16] loss=1.64 avg=1.79\n",
            "[75 | 98.39] loss=1.74 avg=1.79\n",
            "[76 | 99.63] loss=1.65 avg=1.78\n",
            "[77 | 100.86] loss=1.67 avg=1.78\n",
            "[78 | 102.09] loss=1.68 avg=1.78\n",
            "[79 | 103.33] loss=1.78 avg=1.78\n",
            "[80 | 104.56] loss=1.48 avg=1.77\n",
            "[81 | 105.80] loss=1.66 avg=1.77\n",
            "[82 | 107.04] loss=1.70 avg=1.77\n",
            "[83 | 108.29] loss=1.72 avg=1.77\n",
            "[84 | 109.52] loss=1.65 avg=1.77\n",
            "[85 | 110.75] loss=1.68 avg=1.77\n",
            "[86 | 111.99] loss=1.51 avg=1.76\n",
            "[87 | 113.23] loss=1.53 avg=1.76\n",
            "[88 | 114.46] loss=1.75 avg=1.76\n",
            "[89 | 115.69] loss=1.55 avg=1.75\n",
            "[90 | 116.92] loss=1.75 avg=1.75\n",
            "[91 | 118.15] loss=1.65 avg=1.75\n",
            "[92 | 119.39] loss=1.74 avg=1.75\n",
            "[93 | 120.61] loss=1.44 avg=1.75\n",
            "[94 | 121.84] loss=1.67 avg=1.74\n",
            "[95 | 123.08] loss=1.67 avg=1.74\n",
            "[96 | 124.32] loss=1.65 avg=1.74\n",
            "[97 | 125.55] loss=1.66 avg=1.74\n",
            "[98 | 126.79] loss=1.71 avg=1.74\n",
            "[99 | 128.03] loss=1.67 avg=1.74\n",
            "[100 | 129.26] loss=1.66 avg=1.74\n",
            "======== SAMPLE 1 ========\n",
            " slightly of the spirit which the spirit, and\n",
            "           the spirit of faith, is willing to give unto the Lord, that he may\n",
            "           save his children.\n",
            "\n",
            "43:008:011 And I will make my countenance that it should be pure, and I will\n",
            "            make it that they should be holy, and that both men and\n",
            "            all persons should worship; and I will keep the word of the first\n",
            "            God.\n",
            "\n",
            "43:008:012 And I will make the spirit of the flesh, of the spirit of\n",
            "            the spirit that the people should worship, and of the spirit of\n",
            "            faith: and they shall rise up unto the Lord, and they\n",
            "            shall know that the Spirit which the people had not heard\n",
            "            before was in their heart.\n",
            "\n",
            "43:008:013 And their fathers shall say unto the children of men,\n",
            "            We have no witness that this spirit is in any\n",
            "            place with them: that the spirit of faith which the people have\n",
            "            been baptized with, it may be converted unto the body of the\n",
            "            people.\n",
            "\n",
            "43:008:014 And we will see that all men do good, and that none be hurt:\n",
            "\n",
            "43:008:015 As to all the children that had been baptized, when by the\n",
            "            name of the Lord God they were baptized again, the\n",
            "            things that they had done by the name of faith, as with the\n",
            "            baptism of the people of God being justified.\n",
            "\n",
            "43:008:016 And I will make the spirit of the body, of the body that is\n",
            "            baptized with; that it may be in the blood of their\n",
            "            father: that they may be saved, that they may be born again and\n",
            "            possessings, to be free from the guilt of the flesh.\n",
            "\n",
            "43:008:017 And I will give them the same powers.\n",
            "\n",
            "43:008:018 And I will give them the same things; and I will give to the\n",
            "            people the same powers, and I will give to all persons, that they\n",
            "            may be free from the guilt of the flesh.\n",
            "\n",
            "43:008:019 And I will give them the same things: and I will give them that\n",
            "            which the people have not received because of the power of the\n",
            "            name for which they have be baptized.\n",
            "\n",
            "43:008:020 And I will make them not of themselves, but of others;\n",
            "            and they shall not be judged, but they shall be set free.\n",
            "\n",
            "43:008:021 And I will make them free of all the wickedness of all the enemies\n",
            "           of the Church.\n",
            "\n",
            "43:008.022 And I will bring the people out of their enemies, that they may\n",
            "           be in the peace of God.\n",
            "\n",
            "43:008:023 And I will bring them out of the evil of the world, that\n",
            "           they may be righteous with God.\n",
            "\n",
            "43:008:024 And I will make them to fear the Lord: but the power of the\n",
            "           holy flesh is made manifest, because it shall have eternal\n",
            "           life.\n",
            "\n",
            "43:009:049 This is the first, and the last thing wherein there can be no more\n",
            "           understanding shall arise.\n",
            "\n",
            "43:009:050 The first sign is, that the wicked shall not be called out\n",
            "           unto God; that they might come unto him; but they that are\n",
            "   \n",
            "\n",
            "[101 | 141.80] loss=1.62 avg=1.74\n",
            "[102 | 143.03] loss=1.58 avg=1.73\n",
            "[103 | 144.27] loss=1.78 avg=1.73\n",
            "[104 | 145.50] loss=1.55 avg=1.73\n",
            "[105 | 146.72] loss=1.73 avg=1.73\n",
            "[106 | 147.97] loss=1.61 avg=1.73\n",
            "[107 | 149.19] loss=1.76 avg=1.73\n",
            "[108 | 150.43] loss=1.32 avg=1.72\n",
            "[109 | 151.66] loss=1.61 avg=1.72\n",
            "[110 | 152.89] loss=1.73 avg=1.72\n",
            "[111 | 154.12] loss=1.72 avg=1.72\n",
            "[112 | 155.35] loss=1.52 avg=1.72\n",
            "[113 | 156.58] loss=1.64 avg=1.72\n",
            "[114 | 157.82] loss=1.70 avg=1.72\n",
            "[115 | 159.06] loss=1.68 avg=1.72\n",
            "[116 | 160.29] loss=1.40 avg=1.71\n",
            "[117 | 161.53] loss=1.56 avg=1.71\n",
            "[118 | 162.77] loss=1.62 avg=1.71\n",
            "interrupted\n",
            "Saving checkpoint/run1/model-118\n",
            "19:015:001 And thus said the LORD,\n",
            "\n",
            "19:015:002 I will make the seed of the prophets of Israel,\n",
            "\n",
            "19:015:003 and will make them out of the mouth of the north\n",
            "          mountains, and in the valleys of the north, Gedor and\n",
            "          Lamech, and in the mountains of the south,\n",
            "\n",
            "19:015:004 And the prophets' mouth shall be upon them: and they shall\n",
            "          be as the words of the prophet: for he shall not\n",
            "          speak unto them, but shall speak unto them\n",
            "          only as the words of the prophet.\n",
            "\n",
            "19:015:005 And I will make the wilderness before my face,\n",
            "\n",
            "19:015:006 And I will make the wilderness of the water of the sea,\n",
            "           and the land of the swamps, and the land of the\n",
            "          mountains of the deep, and the land of the fjords, and\n",
            "          all the land which is upon the east.\n",
            "\n",
            "19:015:007 And I will make the wilderness of the waters of the sea,\n",
            "          and the land of the mountains of the east, and the land\n",
            "          upon the west, and the earth of the earth, and the mountains\n",
            "          of the hills, and the heaven, and all the heavenly\n",
            "          mountains, and the earth, and all the earth which is upon the\n",
            "          heavens, and all the earth which is upon the earth which is\n",
            "          upon the earth, and upon all the earth which is upon the heaven\n",
            "          which is above, and every one of the earth, and every one\n",
            "          of the waters, and the earth, and all the waters of the\n",
            "          waters, and all the waters which is above, and every one of the\n",
            "          waters which is above, that is, every one of the earth,\n",
            "          that is, every one of the waters which is above, and every\n",
            "          of the waters which is above, that is, every one of the earth, and\n",
            "          every one of the waters which is above\n",
            "          every one of the earth, that is, every one of the earth, and every one\n",
            "          that is above.\n",
            "\n",
            "19:015:001 And the earth shall be as the waters of the sea,\n",
            "\n",
            "19:015:002 And the waters of the sea shall be as the waters of the\n",
            "          water, and the waters of the earth shall be as the\n",
            "          waters of the fjords, and the waters of the mountains,\n",
            "\n",
            "19:015:003 And the earth shall be as the waters of the fjords, and\n",
            "          the earth as the waters of the sea, and the earth as\n",
            "          the waters of the fjords, and the earth as the waters of the\n",
            "          fjords, and the earth as the waters of the fjords.\n",
            "\n",
            "19:015:004 And I will make the mountains of the mountains, and the land\n",
            "          of the fjords, and the land of the swamps, and the land of the\n",
            "          mountains of the deep, and the land of the fjords, and the\n",
            "          land of the mountains of the deep, and the land of the swamps\n",
            "          upon the west, and the land of the fjords, and the land\n",
            "          upon the earth, and the earth which is upon the south,\n",
            "          and the land of the swamps, and the land of the hills, and\n",
            "          all the earth in the heavens, and the earth\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
