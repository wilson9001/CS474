{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DL_Lab6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cksgAH12XRjV"},"source":["# Lab 6: Sequence-to-sequence models\n","\n","## Description:\n","For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n","\n","This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n","\n","## There are two parts of this lab:\n","###  1.   Wiring up a basic sequence-to-sequence computation graph\n","###  2.   Implementing your own GRU cell.\n","\n","\n","An example of my final samples are shown below (more detail in the\n","final section of this writeup), after 150 passes through the data.\n","Please generate about 15 samples for each dataset.\n","\n","<code>\n","And ifte thin forgision forward thene over up to a fear not your\n","And freitions, which is great God. Behold these are the loss sub\n","And ache with the Lord hath bloes, which was done to the holy Gr\n","And appeicis arm vinimonahites strong in name, to doth piseling \n","And miniquithers these words, he commanded order not; neither sa\n","And min for many would happine even to the earth, to said unto m\n","And mie first be traditions? Behold, you, because it was a sound\n","And from tike ended the Lamanites had administered, and I say bi\n","</code>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c2i_QpSsWG4c"},"source":["---\n","\n","## Part 0: Readings, data loading, and high level training\n","\n","---\n","\n","There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n","\n","* Read the following\n","\n","> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n","* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"l7bdZWxvJrsx","outputId":"af0dc660-21bc-438b-b062-4d41e7b5ead7","executionInfo":{"status":"ok","timestamp":1571546122316,"user_tz":360,"elapsed":16153,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}},"colab":{"base_uri":"https://localhost:8080/","height":364}},"source":["! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n","! tar -xzf text_files.tar.gz\n","! pip install unidecode\n","! pip install torch\n","\n","import unidecode\n","import string\n","import random\n","import re\n"," \n","import pdb\n"," \n","all_characters = string.printable\n","n_characters = len(all_characters)\n","file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n","secondFile = unidecode.unidecode(open('./text_files/alma.txt').read())\n","file_len = len(file)\n","secondFileLen = len(secondFile)\n","print('file_len =', file_len)"],"execution_count":83,"outputs":[{"output_type":"stream","text":["--2019-10-20 04:35:07--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n","Resolving piazza.com (piazza.com)... 3.214.17.10, 34.205.95.128, 52.2.48.133, ...\n","Connecting to piazza.com (piazza.com)|3.214.17.10|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n","--2019-10-20 04:35:08--  https://d1b10bmlvqabco.cloudfront.net/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n","Resolving d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)... 52.84.225.14, 52.84.225.66, 52.84.225.86, ...\n","Connecting to d1b10bmlvqabco.cloudfront.net (d1b10bmlvqabco.cloudfront.net)|52.84.225.14|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1533290 (1.5M) [application/x-gzip]\n","Saving to: ‘./text_files.tar.gz’\n","\n","./text_files.tar.gz 100%[===================>]   1.46M  5.30MB/s    in 0.3s    \n","\n","2019-10-20 04:35:09 (5.30 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n","\n","Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.1.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.3.0+cu100)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n","file_len = 2579888\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TxBeKeNjJ0NQ","outputId":"bbb512e5-521c-46f9-e75b-9947abe0b6dc","executionInfo":{"status":"ok","timestamp":1571546122325,"user_tz":360,"elapsed":16133,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["chunk_len = 200\n"," \n","def random_chunk():\n","  start_index = random.randint(0, file_len - chunk_len)\n","  end_index = start_index + chunk_len + 1\n","  return file[start_index:end_index]\n","  \n","print(random_chunk())\n","\n","def random_second_chunk():\n","  startIndex = random.randint(0, secondFileLen - chunk_len)\n","  endIndex = startIndex + chunk_len + 1\n","  return secondFile[startIndex:endIndex]\n","\n","print(random_second_chunk())"],"execution_count":84,"outputs":[{"output_type":"stream","text":["' said Merry. 'Now! Wake all our people! They hate all \n","this, you can see: all of them except perhaps one or two rascals, and a few \n","fools that want to be important, but don't at all understand what is\n","subjects to the devil?\n","\n"," I say unto you, ye will know at that day that ye cannot be saved; for there can no man be saved except his garments are washed white; yea, his garments must be purified until t\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"On0_WitWJ99e","outputId":"24fc8ae9-3548-40b1-d1ae-0052f14349d5","executionInfo":{"status":"ok","timestamp":1571546122335,"user_tz":360,"elapsed":16119,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import torch\n","from torch.autograd import Variable\n","# Turn string into list of longs\n","def char_tensor(string):\n","  tensor = torch.zeros(len(string)).long()\n","  for c in range(len(string)):\n","      tensor[c] = all_characters.index(string[c])\n","  return Variable(tensor)\n","\n","print(char_tensor('abcDEF'))"],"execution_count":85,"outputs":[{"output_type":"stream","text":["tensor([10, 11, 12, 39, 40, 41])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CYJPTLcaYmfI"},"source":["---\n","\n","## Part 4: Creating your own GRU cell \n","\n","**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n","\n","---\n","\n","The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n","\n","Please try not to look at the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n","\n","**TODO:**\n","\n","**DONE:**\n","* Create a custom GRU cell\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aavAv50ZKQ-F","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","\n","\n","class GRU(nn.Module):\n","  def __init__(self, input_size, hidden_size, num_layers):\n","    super(GRU, self).__init__()\n","    self.sigmoid = nn.Sigmoid()\n","    self.tanh = nn.Tanh()\n","    self.inputResetWeights = nn.init.xavier_uniform_(torch.empty(1,input_size))\n","    self.inputResetBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","    self.hiddenResetWeights = nn.init.xavier_uniform_(torch.empty(1, hidden_size))\n","    self.hiddenResetBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","    self.inputUpdateWeights = nn.init.xavier_uniform_(torch.empty(1, input_size))\n","    self.inputUpdateBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","    self.hiddenUpdateWeights = nn.init.xavier_uniform_(torch.empty(1, hidden_size))\n","    self.hiddenUpdateBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","    self.inputNewWeights = nn.init.xavier_uniform_(torch.empty(1, input_size))\n","    self.inputNewBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","    self.hiddenNewWeights = nn.init.xavier_uniform_(torch.empty(1, hidden_size))\n","    self.hiddenNewBias = torch.ones(1, requires_grad=False).fill_(0.8)#.detach()\n","  \n","  def forward(self, inputs, hidden):\n","    inputs = inputs.squeeze(0)\n","    hidden = hidden.squeeze(0)\n","    resetGate = self.sigmoid(torch.mm(self.inputResetWeights, inputs.t()) + self.inputResetBias + torch.mm(self.hiddenResetWeights, hidden.t()) + self.hiddenResetBias)\n","    updateGate = self.sigmoid(torch.mm(self.inputUpdateWeights, inputs.t()) + self.inputUpdateBias + torch.mm(self.hiddenUpdateWeights, hidden.t()) + self.hiddenUpdateBias)\n","    newGate = self.tanh(torch.mm(self.inputNewWeights, inputs.t()) + self.inputNewBias + resetGate * (torch.mm(self.hiddenNewWeights, hidden.t()) + self.hiddenNewBias))\n","    newHiddenLayer = (1 - updateGate) * newGate + updateGate * hidden\n","                                                    \n","    outputs = newHiddenLayer.unsqueeze(0)\n","    hiddens = newHiddenLayer.clone().unsqueeze(0)#detach().unsqueeze()\n","    # Each layer does the following:\n","    # resetGate = Sigmoid(inputResetWeights * inputsTransposed + inputResetBias + hiddenResetWeights * hiddenLayer + hiddenResetBias)\n","    # updateGate = Sigmmoid(inputUpdateWeights * inputsTransposed + inputUpdateBias + hiddenUpdateWeights * hiddenLayer + hiddenUpdateBias)\n","    # newGate = tanh(inputNewWeights * inputsTransposed + inputNewBias + resetGate *elementwise multiplication* (hiddenNewWeights * hiddenLayer + hiddenNewBias))\n","    # newHiddenLayer = (1 - updateGate) *elementwise multiplication* newGate + updateGate *elementwise multiplication* hiddenLayer\n","    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n","    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n","    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n","    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n","    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n","    \n","    # output is copy of last hidden layer, hiddens in array of all hidden states, which in our case is just one hidden state because we are only supporting single-layer GRU's\n","    \n","    return outputs, hiddens\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qtXdX-B_WiAY"},"source":["---\n","\n","##  Part 1: Building a sequence to sequence model\n","\n","---\n","\n","Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n","\n","We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n","\n","\n","**TODO:**\n","\n","**DONE:**\n","* Create an RNN class that extends from nn.Module.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d6tNdEnzWj5F","colab":{}},"source":["class RNN(nn.Module):\n","  # decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","    super(RNN, self).__init__()\n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    self.output_size = output_size\n","    self.n_layers = n_layers\n","    \n","    self.embedding = nn.Embedding(output_size, hidden_size)\n","    \n","    # TODO: Implement this class in the cell above\n","    self.gru = GRU(hidden_size, hidden_size, n_layers) #nn.GRU(hidden_size, hidden_size)\n","    self.linear = nn.Linear(hidden_size, output_size)\n","\n","  def forward(self, input_char):#, hidden):\n","    charEmbedding = self.embedding(input_char).view(1, 1, -1)#.detach()\n","    \n","    gruOutput, newHiddenState = self.gru(charEmbedding, self.hiddenState)\n","    #gruOutput.detach()\n","    #newHiddenState.detach()\n","    probabilityDistribution = self.linear(gruOutput[0])\n","    self.hiddenState = newHiddenState\n","    \n","    return probabilityDistribution#.detach() #, newHiddenState\n","\n","  def init_hidden(self):\n","    self.hiddenState = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hrhXghEPKD-5","colab":{}},"source":["def random_training_set():    \n","  chunk = random_chunk()\n","  inp = char_tensor(chunk[:-1]).detach()\n","  target = char_tensor(chunk[1:]).detach()\n","  return inp, target\n","\n","def random_second_training_set():\n","  chunk = random_second_chunk()\n","  inp = char_tensor(chunk[:-1]).detach()\n","  target = char_tensor(chunk[1:]).detach()\n","  return inp, target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZpiGObbBX0Mr"},"source":["---\n","\n","## Part 2: Sample text and Training information\n","\n","---\n","\n","We now want to be able to train our network, and sample text after training.\n","\n","This function outlines how training a sequence style network goes. \n","\n","**TODO:**\n","\n","**DONE:**\n","* Fill in the pieces.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2ALC3Pf8Kbsi","colab":{}},"source":["def train(inp, target):\n","   \n","  # Turn input and target characters into indices using character list\n","  inputIndexTensor = inp#.detach() #char_tensor(inp)\n","  targetIndexTensor = target#.detach() #char_tensor(target)\n","  \n","  ## initialize hidden layers, set up gradient and loss\n","  decoder_optimizer.zero_grad()\n","  # Hidden layer is initialized along with RNN, hidden state tracked internally so there's no need to return it here.\n","  decoder.init_hidden()\n","  loss = 0\n","\n","  # Feed input characters through RNN, get out probability distributions\n","  for tensorIndex in range(len(inputIndexTensor)):\n","    probabilityDistributionForNextChar = decoder(inputIndexTensor[tensorIndex])\n","    \n","    # Compute loss\n","    loss = criterion(probabilityDistributionForNextChar, targetIndexTensor[tensorIndex].unsqueeze(0))\n","    loss += loss.item()\n","  \n","  # Backprop error and adjust\n","  loss.backward()\n","  decoder_optimizer.step()\n","    \n","  return loss/len(inputIndexTensor)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EN06NUu3YRlz"},"source":["---\n","\n","## Part 3: Sample text and Training information\n","\n","---\n","\n","You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n","\n","If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n","\n","**TODO:**\n","\n","**DONE:**\n","* Fill out the evaluate function to generate text frome a primed string"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B-bp-OZ1KjNh","colab":{}},"source":["def distributionToChar(probabilityDistribution):\n","  probabilityDistribution = F.softmax(probabilityDistribution, dim=1)\n","  _, greatestProbabilityIndex = probabilityDistribution.data.topk(1)\n","  _.detach()\n","  charIndex = greatestProbabilityIndex.squeeze().detach()\n","  return all_characters[charIndex.item()], charIndex\n","\n","def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n","  ## initialize hidden variable, initialize other useful variables\n","  decoder.init_hidden()\n","  with torch.no_grad():\n","    inputIndexTensor = char_tensor(prime_str)\n","      \n","    # prep RNN to predict text by giving it all input characters\n","    for inputIndex in range(len(inputIndexTensor)):\n","      distribution = decoder(inputIndexTensor[inputIndex])\n","        \n","    generatedText = [] \n","    predictedChar, nextInputIndex = distributionToChar(distribution)\n","      \n","    generatedText.append(predictedChar)\n","      \n","    for predictionCount in range(predict_len - 1):\n","      distribution = decoder(nextInputIndex)\n","        \n","      predictedChar, nextInputIndex = distributionToChar(distribution)\n","      generatedText.append(predictedChar)\n","       \n","  return generatedText\n","  ## /\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Du4AGA8PcFEW"},"source":["---\n","\n","## Part 4: (Create a GRU cell, requirements above)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GFS2bpHSZEU6"},"source":["\n","---\n","\n","## Part 5: Run it and generate some text!\n","\n","---\n","\n","Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs gave.\n","\n","**TODO:** \n","\n","**DONE:**\n","* Create some cool output\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-nXFeCmdKodw","colab":{}},"source":["import time\n","#n_epochs = 5000\n","print_every = 200\n","plot_every = 10\n","hidden_size = 200\n","n_layers = 1\n","lr = 0.001\n","\n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n"," \n","start = time.time()\n","all_losses = []\n","loss_avg = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xKfozqw-6eqb","colab":{"base_uri":"https://localhost:8080/","height":558},"outputId":"15592ae3-82fe-4105-af1a-faabc7a4802c","executionInfo":{"status":"ok","timestamp":1571547330501,"user_tz":360,"elapsed":341868,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}}},"source":["n_epochs = 2000\n","for epoch in range(1, n_epochs + 1):\n","  loss_ = train(*random_training_set())\n","  loss_avg += loss_\n","\n","  if epoch % print_every == 0:\n","      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","      print(evaluate('Wh', 100), '\\n')\n","\n","  if epoch % plot_every == 0:\n","      all_losses.append(loss_avg / plot_every)\n","      loss_avg = 0"],"execution_count":108,"outputs":[{"output_type":"stream","text":["[38.24638271331787 (200 10%) 0.0321]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'i', ' ', ' ', ' ', ' ', ' ', ' ', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o', 'o'] \n","\n","[72.0439178943634 (400 20%) 0.0130]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'i', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[106.26158022880554 (600 30%) 0.0327]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'i', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'i', 'n', 'n', 'n', 'n', 'i', 'n', 'n'] \n","\n","[139.07024002075195 (800 40%) 0.0267]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'i', 'i', ' ', ' ', ' ', ' ', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', ' ', 't', ' ', 't', 't', ' ', 't'] \n","\n","[172.94177174568176 (1000 50%) 0.0378]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'i', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', ' ', 't', 't', ' ', 't', 't'] \n","\n","[206.6834397315979 (1200 60%) 0.0194]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'i', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[240.16269278526306 (1400 70%) 0.0207]\n","['Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', 'Q', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[274.44314789772034 (1600 80%) 0.0232]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[307.0800075531006 (1800 90%) 0.0195]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[340.46771812438965 (2000 100%) 0.0197]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ee0so6aKJ5L8","colab":{"base_uri":"https://localhost:8080/","height":558},"outputId":"481f4e16-c245-49a3-d624-a564bd4022f0","executionInfo":{"status":"ok","timestamp":1571547332027,"user_tz":360,"elapsed":343366,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}}},"source":["for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"],"execution_count":109,"outputs":[{"output_type":"stream","text":[" ra\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," ca\n","['Q', 'Q', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," wh\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," wh\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," G\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," lo\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," I \n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," lo\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's', 's'] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YJhgDc2IauPE"},"source":["---\n","\n","## Part 6: Generate output on a different dataset\n","\n","---\n","\n","**TODO:**\n","\n","**DONE:**\n","* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n","\n","* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n"]},{"cell_type":"code","metadata":{"id":"kmKPdhtzX543","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"cb519d15-c2c6-4456-fdd7-a53ef3fb3c3f","executionInfo":{"status":"ok","timestamp":1571547666869,"user_tz":360,"elapsed":678183,"user":{"displayName":"nullPointerException","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBhU8NlqWD46_YZHAXtWhrCMDTOotD6khLoUk0_Og=s64","userId":"12424731692871738664"}}},"source":["decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","\n","start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","  loss_ = train(*random_second_training_set())\n","  loss_avg += loss_\n","\n","  if epoch % print_every == 0:\n","      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","      print(evaluate('Wh', 100), '\\n')\n","\n","  if epoch % plot_every == 0:\n","      all_losses.append(loss_avg / plot_every)\n","      loss_avg = 0\n","      \n","for i in range(10):\n","  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","  start = random.randint(0,len(start_strings)-1)\n","  print(start_strings[start])\n","#   all_characters.index(string[c])\n","  print(evaluate(start_strings[start], 200), '\\n')"],"execution_count":110,"outputs":[{"output_type":"stream","text":["[39.467225790023804 (200 10%) 0.0368]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[73.25917077064514 (400 20%) 0.0303]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n","[105.75647354125977 (600 30%) 0.0232]\n","['e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e'] \n","\n","[138.6468756198883 (800 40%) 0.0221]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' '] \n","\n","[171.47968435287476 (1000 50%) 0.0316]\n","[' ', ' ', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' ', ' ', 'A', ' ', ' ', ' '] \n","\n","[203.43421411514282 (1200 60%) 0.0374]\n","[' ', ' ', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'E'] \n","\n","[237.1984691619873 (1400 70%) 0.0175]\n","[' ', ' ', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'w', ' ', ' ', ' ', ' ', 'E', ' ', ' ', '[', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'w', ' ', ' ', ' ', ' ', 'E', ' ', ' ', '[', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'w', ' ', ' ', ' ', ' ', 'E', ' ', ' ', '[', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'w', ' ', ' ', ' ', ' ', 'E', ' ', ' ', '[', ' ', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', ' ', 'w', ' ', ' ', ' ', ' ', 'E', ' ', ' ', '[', ' ', ' ', ' ', ' ', 'E', ' '] \n","\n","[269.75048089027405 (1600 80%) 0.0226]\n","[' ', ' ', ' ', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' ', ' ', 'E', ' ', ' ', 'E', ' ', 'w', ' ', ' '] \n","\n","[302.4306216239929 (1800 90%) 0.0298]\n","[' ', ' ', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' ', ' ', 'E', ' ', 'E', ' ', 'A', ' ', ' '] \n","\n","[334.7391459941864 (2000 100%) 0.0287]\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," lo\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," ca\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," wh\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," he\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," he\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," Th\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"," ra\n","[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' '] \n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ngPdqaSCkY62","colab_type":"text"},"source":["### Model Performance\n","So this model's performance level is that it generates a variety of characters, with no coherence.\n","\n","Strengths:\n","* It generates characters with a sort of variety\n","* It generates the number of characters you ask for\n","* It seems to be sort of learning something\n","\n","Weaknesses:\n","* It doesn't generate coherent words\n","  * The model seems to get stuck predicting one particular character for many other characters. I'm not sure what's causing this, as I played around with the learning rate and was sure to use Xavier initialization in the GRU. I also used Softmax in the probability vector.\n","  * While it bugs me that this net doesn't produce better output, I spent literally my entire Saturday and several hours on Friday working on this, so I don't have any more time to devote to this. In addition, Dr. Wingate said in class that this assignment isn't graded based on the quality of output, and that as long as the network generates something we're good for grading. So I think I've met the requirements for this lab, and if I had more time to make this better I would keep trying to improve this, but I don't have any more time."]}]}